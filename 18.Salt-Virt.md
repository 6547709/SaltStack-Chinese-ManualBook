# Salt Virt

Salt Virt云控制器功能最初是作为Alpha技术添加到版本0.14.0中的Salt中的。

最初的Salt Virt系统支持以下核心云操作：
- 虚拟机部署
- 检查已部署的VM
- 虚拟机迁移
- 网络分析
- 自动将VM与Salt的所有方面集成
- 镜像预配置

当前正在开发许多功能，以增强Salt Virt系统的功能。

> 注意
>
> 值得注意的是，Salt最初就是为了将Salt通信系统用作云控制器的骨干而开发的。 这意味着，Salt Virt系统不是事后才想到的，而是一个作为他功能开发的支撑系统。 开发Salt的云控制方面的最初尝试是一个名为butter的项目。 该项目并未取得成功，但发挥了作用，证明了Salt作为云控制器的早期可行性。

> 警告
>
> Salt Virt不适用于管理在VM中运行的KVM。 KVM必须是直接在祼机硬件上运行的。


## Salt Virt Tutorial
在教程部分中有一个有关如何启动和运行Salt Virt的教程：

[Cloud Controller Tutorial](#Cloud-Controller-Tutorial)

## The Salt Virt Runner

与cloud controller交互的是virt runner程序。 virt runner程序附带有执行特定虚拟机例程的例程。

Runner模块文档中提供了virt runner的详细参考文档：

[Virt Runner Reference](https://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.virt.html#module-salt.modules.virt)

## Based on Live State Data

Salt Virt系统基于使用Salt查询有关虚拟机管理程序的实时数据，然后使用收集的数据来做出有关云操作的决策的基础。 这意味着运行Salt Virt不需要外部资源，并且收集的有关云的信息是实时且准确的。

## Deploy from Network or Disk

### Virtual Machine Disk Profiles - 虚机的磁盘配置

Salt Virt允许对为部署的虚拟机创建的磁盘进行精细配置。 配置是一个简单的数据结构，可从`config.option`函数读取，这意味着该配置可以存储在minion配置文件、master配置文件或minion的pillar中。

此配置选项称为`virt.disk`。 默认的`virt.disk`数据结构如下所示：
```yaml
virt.disk:
  default:
    - system:
      size: 8192
      format: qcow2
      model: virtio
```

> 注意
>
> 格式和模型均不需要定义，Salt将默认为基础虚拟机管理程序使用的最佳格式，在kvm的情况下，它是qcow2和virtio。

此配置将设置一个名为default的磁盘配置文件。 该配置文件将在虚拟机上创建一个系统磁盘。

#### Define More Profiles - 定义和使用更多的磁盘配置方案

许多环境将需要使用更复杂的磁盘配置文件，并且可能需要多个配置文件，而这可以轻松实现：
```yaml
virt.disk:
  default:
    - system:
        size: 8192
  database:
    - system:
        size: 8192
    - data:
        size: 30720
  web:
    - system:
        size: 1024
    - logs:
        size: 5120
```

以上配置允许在部署虚机时选择使用这三个配置方案之一，从而可以根据部署的虚拟机的不同存储需求创建虚拟机。

### Virtual Machine Network Profiles - 虚机的网络配置

Salt Virt允许对为部署的虚拟机创建的网络设备进行精细配置。 配置是一个简单的数据结构，可从`config.option`函数读取，这意味着该配置可以存储在minion配置文件、master配置文件或minion的pillar中。

此配置选项称为`virt: nic`。 默认情况下，`virt: nic`选项为空，其默认的数据结构如下所示：
```yaml
virt:
  nic:
    default:
      eth0:
        bridge: br0
        model: virtio
```

> 注意

无需定义驱动模型，Salt将默认为基础虚拟机管理程序使用最佳的驱动模型，对于kvm而言，此驱动模型为virtio

此配置设置了一个名为default的网络配置方案。 默认配置文件在虚拟机上创建单个以太网设备，该设备桥接到虚拟机管理程序的`br0`接口。 此默认设置不需要设置`virt: nic`配置，这就是为什么默认安装仅需要在系统管理程序上设置`br0`桥接设备的原因。

#### Define More Profiles - 更多的网络配置方案

许多环境将需要更复杂的网络配置方案，并且可能需要多个配置文件，这很容易实现：
```yaml
virt:
  nic:
    dual:
      eth0:
        bridge: service_br
      eth1:
        bridge: storage_br
    single:
      eth0:
        bridge: service_br
    triple:
      eth0:
        bridge: service_br
      eth1:
        bridge: storage_br
      eth2:
        bridge: dmz_br
    all:
      eth0:
        bridge: service_br
      eth1:
        bridge: storage_br
      eth2:
        bridge: dmz_br
      eth3:
        bridge: database_br
    dmz:
      eth0:
        bridge: service_br
      eth1:
        bridge: dmz_br
    database:
      eth0:
        bridge: service_br
      eth1:
        bridge: database_br
```
该配置文件中允许选择六个配置方案之一，从而允许创建虚拟机，这些虚拟机根据部署的vm的需要连接到不同的网络中。


### Salt as a Cloud Controller - 将Salt作为一个云平台控制器

在Salt 0.14.0中，引入了高级的cloud controll系统，该系统允许使用Salt直接管理私有云虚拟机。该系统通常称为"Salt Virt"。

Salt Virt系统已经存在并已安装在Salt自身中，这意味着除了设置Salt之外，无需部署其他Salt代码。

> 注意
>
> 需要依赖于`libvirt` python模块和`certtool`二进制文件。

Salt Virt的主要目标是提升云平台的简易性和使用效率。可以扩展并具有云平台的完整管理功能。 Salt Virt具有设置和管理复杂虚拟机网络、强大的镜像和磁盘管理以及使用或不使用共享存储的虚拟机迁移的功能。

这意味着Salt Virt可用于从刀片服务器和SAN存储创建云主机，但是也可以在没有单个共享存储系统的情况下从大量Linux桌面中创建云。 Salt Virt可以用真正的商业硬件创建云，也可以支持使用很多的专用硬件。

#### Setting up Hypervisors - 设置虚拟机监控程序

设置系统管理程序的第一步涉及安装正确的软件并设置系统管理程序网络接口。

##### Installing Hypervisor Software - 安装虚拟机管理程序

`Salt Virt`被确定为与虚拟机管理程序无关，但是目前唯一完全实现的虚拟机管理程序是通过`libvirt`运行的`KVM`。

系统管理程序所需的软件是`libvirt`和`kvm`。 要获得高级功能，请安装`libguestfs`或`qemu-nbd`。

> 注意
>
> `Libguestfs`和`qemu-nbd`允许在启动虚机之前挂载虚拟机镜像，并进行预配置和预置一个Salt minion。

下面的sls将为管理程序设置所需的软件，并运行相关例程以设置libvirt pki密钥。

> 注意
>
> 下面的软件包名称和使用的设置是特定于Red Hat的，不同平台将需要不同的软件包名称

```yaml
libvirt:
  pkg.installed: []
  file.managed:
    - name: /etc/sysconfig/libvirtd
    - contents: 'LIBVIRTD_ARGS="--listen"'
    - require:
      - pkg: libvirt
  virt.keys:
    - require:
      - pkg: libvirt
  service.running:
    - name: libvirtd
    - require:
      - pkg: libvirt
      - network: br0
      - libvirt: libvirt
    - watch:
      - file: libvirt

libvirt-python:
  pkg.installed: []

libguestfs:
  pkg.installed:
    - pkgs:
      - libguestfs
      - libguestfs-tools
```

##### Hypervisor Network Setup - 设置虚拟机管理程序的网络

虚拟机管理程序将需要运行网桥以为虚拟机提供网络设备，下面的sls公式将在虚拟机管理程序上建立标准网桥，并将网桥连接到eth0：
```yaml
eth0:
  network.managed:
    - enabled: True
    - type: eth
    - bridge: br0

br0:
  network.managed:
    - enabled: True
    - type: bridge
    - proto: dhcp
    - require:
      - network: eth0
```

##### Virtual Machine Network Setup - 设置虚拟机的网络

Salt Virt带有一个系统，可以对已部署的虚拟机使用的网络接口进行建模。 默认情况下，将为已部署的虚拟机创建一个接口，并将其桥接到br0。 要使用默认的网络设置，请确保虚拟机管理程序上存在名为br0的桥接接口并将其桥接到活动的网络设备。

> 注意
>
> 要在Salt Virt中使用更高级的联网，请阅读Salt Virt Networking文档：

[Salt Virt Networking](#Virtual-Machine-Network-Profiles---虚机的网络配置)

##### Libvirt State

部署基于`libvirt`的云的挑战之一是`libvirt`证书的分发。 这些证书允许虚拟机迁移。 Salt附带了用于自动部署这些证书的系统。 Salt管理签名授权密钥，为主机上的`libvirt`客户端生成密钥，并使用CA证书对其进行签名，使用pillar进行分发。 这通过`libvirt states`状态进行管理。 只需在minion上执行以下sls公式即可确保证书分发到位并且是最新的：

> 注意
>
> 上面的sls公式已经包括了设置libvirt密钥所需的调用。

```yaml
libvirt_keys:
  virt.keys
```

#### Getting Virtual Machine Images Ready

Salt Virt, requires that virtual machine images be provided as these are not generated on the fly. Generating these virtual machine images differs greatly based on the underlying platform.

Virtual machine images can be manually created using KVM and running through the installer, but this process is not recommended since it is very manual and prone to errors.

Virtual Machine generation applications are available for many platforms:

kiwi: (openSUSE, SLES, RHEL, CentOS)

    https://suse.github.io/kiwi/
vm-builder:

    https://wiki.debian.org/VMBuilder

    See also

    vmbuilder-formula

Once virtual machine images are available, the easiest way to make them available to Salt Virt is to place them in the Salt file server. Just copy an image into /srv/salt and it can now be used by Salt Virt.

For purposes of this demo, the file name centos.img will be used.

##### Existing Virtual Machine Images

Many existing Linux distributions distribute virtual machine images which can be used with Salt Virt. Please be advised that NONE OF THESE IMAGES ARE SUPPORTED BY SALTSTACK.

**CentOS**

These images have been prepared for OpenNebula but should work without issue with Salt Virt, only the raw qcow image file is needed: http://wiki.centos.org/Cloud/OpenNebula

**Fedora Linux**

Images for Fedora Linux can be found here: https://alt.fedoraproject.org/cloud

**openSUSE**

http://download.opensuse.org/repositories/openSUSE:/Leap:/42.1:/Images/images

(look for JeOS-for-kvm-and-xen variant)

**SUSE**

https://www.suse.com/products/server/jeos

**Ubuntu Linux**

Images for Ubuntu Linux can be found here: http://cloud-images.ubuntu.com/

#### Using Salt Virt

With hypervisors set up and virtual machine images ready, Salt can start issuing cloud commands using the virt runner.

Start by running a Salt Virt hypervisor info command:

salt-run virt.host_info

This will query the running hypervisor(s) for stats and display useful information such as the number of cpus and amount of memory.

You can also list all VMs and their current states on all hypervisor nodes:

salt-run virt.list

Now that hypervisors are available a virtual machine can be provisioned. The virt.init routine will create a new virtual machine:

salt-run virt.init centos1 2 512 salt://centos.img

The Salt Virt runner will now automatically select a hypervisor to deploy the new virtual machine on. Using salt:// assumes that the CentOS virtual machine image is located in the root of the Salt File Server on the master. When images are cloned (i.e. copied locatlly after retrieval from the file server) the destination directory on the hypervisor minion is determined by the virt:images config option; by default this is /srv/salt-images/.

When a VM is initialized using virt.init the image is copied to the hypervisor using cp.cache_file and will be mounted and seeded with a minion. Seeding includes setting pre-authenticated keys on the new machine. A minion will only be installed if one can not be found on the image using the default arguments to seed.apply.

Note

The biggest bottleneck in starting VMs is when the Salt Minion needs to be installed. Making sure that the source VM images already have Salt installed will GREATLY speed up virtual machine deployment.

You can also deploy an image on a particular minion by directly calling the virt execution module with an absolute image path. This can be quite handy for testing:

salt 'hypervisor*' virt.init centos1 2 512 image=/var/lib/libvirt/images/centos.img

Now that the new VM has been prepared, it can be seen via the virt.query command:

salt-run virt.query

This command will return data about all of the hypervisors and respective virtual machines.

Now that the new VM is booted it should have contacted the Salt Master, a test.version will reveal if the new VM is running.
QEMU copy on write support

For fast image cloning you can use the qcow disk image format. Pass the enable_qcow flag and a .qcow2 image path to virt.init:

salt 'hypervisor*' virt.init centos1 2 512 image=/var/lib/libvirt/images/centos.qcow2 enable_qcow=True start=False

Note

Beware that attempting to boot a qcow image too quickly after cloning can result in a race condition where libvirt may try to boot the machine before image seeding has completed. For that reason it is recommended to also pass start=False to virt.init.

Also know that you must not modify the original base image without first making a copy and then rebasing all overlay images onto it. See the qemu-img rebase usage docs.
Migrating Virtual Machines

Salt Virt comes with full support for virtual machine migration, and using the libvirt state in the above formula makes migration possible.

A few things need to be available to support migration. Many operating systems turn on firewalls when originally set up, the firewall needs to be opened up to allow for libvirt and kvm to cross communicate and execution migration routines. On Red Hat based hypervisors in particular port 16514 needs to be opened on hypervisors:

iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 16514 -j ACCEPT

Note

More in-depth information regarding distribution specific firewall settings can read in:

Opening the Firewall up for Salt

Salt also needs the virt:tunnel option to be turned on. This flag tells Salt to run migrations securely via the libvirt TLS tunnel and to use port 16514. Without virt:tunnel libvirt tries to bind to random ports when running migrations.

To turn on virt:tunnel simply apply it to the master config file:

virt:
    tunnel: True

Once the master config has been updated, restart the master and send out a call to the minions to refresh the pillar to pick up on the change:

salt \* saltutil.refresh_modules

Now, migration routines can be run! To migrate a VM, simply run the Salt Virt migrate routine:

salt-run virt.migrate centos <new hypervisor>

VNC Consoles

Although not enabled by default, Salt Virt can also set up VNC consoles allowing for remote visual consoles to be opened up. When creating a new VM using virt.init pass the enable_vnc=True parameter to have a console configured for the new VM.

The information from a virt.query routine will display the vnc console port for the specific vms:

centos
  CPU: 2
  Memory: 524288
  State: running
  Graphics: vnc - hyper6:5900
  Disk - vda:
    Size: 2.0G
    File: /srv/salt-images/ubuntu2/system.qcow2
    File Format: qcow2
  Nic - ac:de:48:98:08:77:
    Source: br0
    Type: bridge

The line Graphics: vnc - hyper6:5900 holds the key. First the port named, in this case 5900, will need to be available in the hypervisor's firewall. Once the port is open, then the console can be easily opened via vncviewer:

vncviewer hyper6:5900

By default there is no VNC security set up on these ports, which suggests that keeping them firewalled and mandating that SSH tunnels be used to access these VNC interfaces. Keep in mind that activity on a VNC interface that is accessed can be viewed by any other user that accesses that same VNC interface, and any other user logging in can also operate with the logged in user on the virtual machine.
Conclusion

Now with Salt Virt running, new hypervisors can be seamlessly added just by running the above states on new bare metal machines, and these machines will be instantly available to Salt Virt.


https://docs.saltstack.com/en/latest/topics/virt/index.html
